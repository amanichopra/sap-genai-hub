{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e97e030",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/amanichopra/sap-genai-hub/blob/main/orchestration_templating.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c87e6-ce3e-418a-811a-58edb1793d1d",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd414455",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d234a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"generative-ai-hub-sdk[all]\"\n",
    "!pip install \"numpy<2.0.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52264258",
   "metadata": {},
   "source": [
    "Now, make sure to reset the runtime. In Google Colab, you can do this by clicking `Runtime` and `Restart Session`, as shown here:\n",
    "\n",
    "<img src=\"assets/colab_restart_session.png\" style=\"width:500px\">\n",
    "\n",
    "Now, you can continue by running the below cells. The packages have already been installed into the runtime before restarting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee09bfd-3ac5-4eb5-85a4-902ceb22cc27",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da77e21-8ce3-462a-82bd-9b455f4b70e4",
   "metadata": {},
   "source": [
    "Before requests to orchestration can be issued, we need to provide authentication details to the SDK. This can be done either via a configuration file or via the environment. Make sure to read the [Generative AI Hub SDK docs](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/index.html) for more details. Below you will find an example for authenticating via environment variables using this very notebook. Ensure to store credentials in a file called `env_vars.env` file for the below command to work. If using Google Colab, you can place this file in the project folder by clicking the folder icon on the left and dropping the file in the workspace as shown:\n",
    "\n",
    "<img src=\"./assets/upload_env.png\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a9c85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path='env_vars.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f1dfa-cd15-452c-a97e-23df5c546eef",
   "metadata": {},
   "source": [
    "## Intializing the Orchestration Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d569326-4f43-4bae-a364-7d766ec07bcf",
   "metadata": {},
   "source": [
    "Typically, a virtual deployment of Orchestration must be configured before any interactions can occur. Once deployed, you will have access to a unique endpoint URL and deployment ID. You can use either the URL or ID when using the SDK. In this particular exercise, we will use deployment ID, which should be defined the the `AICORE_ORCH_DEPLOYMENT_ID` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac688975-c661-49b6-9bb0-6b9e3d154b49",
   "metadata": {},
   "source": [
    "# Templating Module in the Orchestration Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be943bca-8929-4985-8599-428734792bc7",
   "metadata": {},
   "source": [
    "Now that everything is prepared, we can write our first basic orchestration pipeline. The first fundamental module we will look at is templating. The templating module provides capabilities to define prompt skeletons that can then be parameterized per inference call. To check out how this works we first up select a Large Language Model (LLM) that will be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "311a7151-e10a-4e95-b03a-92047c077215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.llm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    name=\"gemini-1.5-flash\",\n",
    "    version=\"latest\",\n",
    "    parameters={\"max_tokens\": 256, \"temperature\": 0.2},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f817af4-fece-4d7b-afdb-0c6088940857",
   "metadata": {},
   "source": [
    "Now we can create a template using the template object provided by the Generative AI Hub SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab7da4b-47a4-408e-bbed-598323e4c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage\n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue\n",
    "\n",
    "template = Template(\n",
    "    messages=[\n",
    "        SystemMessage(\"You are a helpful translation assistant.\"),\n",
    "        UserMessage(\n",
    "            \"Translate the following text to {{?to_lang}}: {{?text}}\",\n",
    "        ),\n",
    "    ],\n",
    "    defaults=[\n",
    "        TemplateValue(name=\"to_lang\", value=\"English\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522e4cf-fcce-4219-a02d-ce94a2fd0443",
   "metadata": {},
   "source": [
    "The code above creates a template that provides\n",
    "\n",
    "- a system message,\n",
    "- a user message that leverages templating syntax,\n",
    "- default values for the introduced template parameters.\n",
    "\n",
    "Currently there are three message types available:\n",
    "- `SystemMessage`: A message for priming AI behavior. The system message is usually passed in as the first of a sequence of input messages.\n",
    "- `UserMessage`: A message from a user.\n",
    "- `AssistantMessage`: A message of the LLM.\n",
    "\n",
    "Parameters are defined within the message string using the following syntax: `{{?param_name}}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f116aab-49fd-4161-934c-b675172626e4",
   "metadata": {},
   "source": [
    "Next up we create a orchestration configuration from the created objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99af79b8-1f77-4244-8528-40f1a8864e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig\n",
    "\n",
    "config = OrchestrationConfig(\n",
    "    template=template,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c83f9-a3bd-4e46-8c8b-3388b672cbad",
   "metadata": {},
   "source": [
    "Lastly, we can call the orchestration service. Note that the actual template values are now passed to the `run` method. The `TemplateValue` name parameter corresponds to the parameter name `text` provided in the user message string. The parameter `to_lang` is omitted and the default defined in the PromptTemplate is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf112f6-ba4a-4fe6-8c52-28015d81d1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive learning with SAP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.orchestration.service import OrchestrationService\n",
    "\n",
    "orchestration_service = OrchestrationService(\n",
    "    deployment_id=os.environ['AICORE_ORCH_DEPLOYMENT_ID'],\n",
    "    config=config,\n",
    ")\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"text\",\n",
    "            value=\"Interaktives Lernen mit SAP.\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb096e-68d8-4e73-bf46-74531128b612",
   "metadata": {},
   "source": [
    "# Model Harmonization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fc5aa-07f5-41b6-aba8-15527d833b7b",
   "metadata": {},
   "source": [
    "Orchestration harmonizes model usage, removing the need for prompting each model in vendor specific fashion. You can easily switch between a variety of models. Check out this SAP [note](https://me.sap.com/notes/3437766) for further information regarding model availability. The code below will demonstrate how to easily switch between models, based on the templating code above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "025870d1-fd8e-4330-ad5b-79ba5214f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive learning with SAP.\n"
     ]
    }
   ],
   "source": [
    "orchestration_service.config.llm = LLM(\n",
    "    name=\"o3-mini\",\n",
    ") # switch out gemini-1.5-flash with o3-mini\n",
    "\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"text\",\n",
    "            value=\"Interaktives Lernen mit SAP.\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f58ab-9954-4010-a884-ca045ebee737",
   "metadata": {},
   "source": [
    "You can switch between those two models and compare their responses throughout all exercises. Simply change the `name` parameter of the LLM module configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bc8ae-4ed2-468a-9094-7c842bd22ee8",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae4510-d8b7-41cc-b8c6-9b7268137411",
   "metadata": {},
   "source": [
    "Within this exercise you learned how to create a basic Orchestration pipeline that uses the Templating module. Also, you changed the model used for inference with ease. Let's explore more modules in the following exercises. Continue to [Exercise 3 - Orchestration Content Filtering](./orchestration_content_filtering.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
