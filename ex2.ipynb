{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637c87e6-ce3e-418a-811a-58edb1793d1d",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee09bfd-3ac5-4eb5-85a4-902ceb22cc27",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da77e21-8ce3-462a-82bd-9b455f4b70e4",
   "metadata": {},
   "source": [
    "Before requests to orchestration can be issued, we need to provide authentication details to the SDK. This can be done either via a configuration file or via the environment. Make sure to check out the [SAP generative AI hub SDK project description](https://pypi.org/project/generative-ai-hub-sdk/) for more details. Below you will find an example for authenticating via environment variables using this very notebook.\n",
    "\n",
    "> **WARNING:**\n",
    "> Below code should never be used in production scenarios and is only for the purpose of illustrating which environment variables to use!\n",
    "> Credentials should never be defined in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3fe1cc2-ceb1-4a07-aad2-b711674b4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AICORE_AUTH_URL\"] = \"https://saproxyshared.authentication.eu12.hana.ondemand.com/oauth/token\"\n",
    "os.environ[\"AICORE_BASE_URL\"] = \"redacted\"\n",
    "os.environ[\"AICORE_CLIENT_ID\"] = \"sb-na-7534ccc9-f8be-46e8-85e8-3e1a7afc2e08!b631918\"\n",
    "os.environ[\"AICORE_CLIENT_SECRET\"] = \"3c202202-3aad-46bc-b8b1-d3edf74b9407$IaIcyD5KfCsOFDlOEpL9pgOdkf5aSs-iqomIKdw7uoM=\"\n",
    "os.environ[\"AICORE_ORCHESTRATION_DEPLOYMENT_URL\"] = \"https://aicore-proxy-ai180p104road.cfapps.eu12.hana.ondemand.com/v2/inference/deployments/d809af1946272325\"\n",
    "os.environ[\"AICORE_RESOURCE_GROUP\"] = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c63705-6c9f-4cc0-ad6a-3dd88e6ce860",
   "metadata": {},
   "source": [
    "You will need to authenticate in every exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f1dfa-cd15-452c-a97e-23df5c546eef",
   "metadata": {},
   "source": [
    "## Intializing the Orchestration Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d569326-4f43-4bae-a364-7d766ec07bcf",
   "metadata": {},
   "source": [
    "Typically, a virtual deployment of Orchestration must be configured before any interactions can occur. Once deployed, you will have access to a unique endpoint URL. For this session, the deployment has already been created for you. The corresponding URL, along with the authentication credentials, have been provided to you in advance. If you've adhered to the prerequisite guidelines, the URL should already be present in the environment variable `AICORE_ORCHESTRATION_DEPLOYMENT_URL`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac688975-c661-49b6-9bb0-6b9e3d154b49",
   "metadata": {},
   "source": [
    "# Templating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be943bca-8929-4985-8599-428734792bc7",
   "metadata": {},
   "source": [
    "Now that everything is prepared, we can write our first basic orchestration pipeline. The first fundamental module we will look at is templating. The templating module provides capabilities to define prompt skeletons that can then be parameterized per inference call. To check out how this works we first up select a Large Language Model (LLM) that will be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311a7151-e10a-4e95-b03a-92047c077215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.llm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    name=\"gemini-1.5-flash\",\n",
    "    version=\"latest\",\n",
    "    parameters={\"max_tokens\": 256, \"temperature\": 0.2},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f817af4-fece-4d7b-afdb-0c6088940857",
   "metadata": {},
   "source": [
    "Now we can create a template using the template object provided by the SAP generative AI hub SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab7da4b-47a4-408e-bbed-598323e4c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage\n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue\n",
    "\n",
    "template = Template(\n",
    "    messages=[\n",
    "        SystemMessage(\"You are a helpful translation assistant.\"),\n",
    "        UserMessage(\n",
    "            \"Translate the following text to {{?to_lang}}: {{?text}}\",\n",
    "        ),\n",
    "    ],\n",
    "    defaults=[\n",
    "        TemplateValue(name=\"to_lang\", value=\"English\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522e4cf-fcce-4219-a02d-ce94a2fd0443",
   "metadata": {},
   "source": [
    "The code above creates a template that provides\n",
    "\n",
    "- a system message,\n",
    "- a user message that leverages templating syntax,\n",
    "- default values for the introduced template parameters.\n",
    "\n",
    "Currently there are three message types available:\n",
    "- `SystemMessage`: A message for priming AI behavior. The system message is usually passed in as the first of a sequence of input messages.\n",
    "- `UserMessage`: A message from a user.\n",
    "- `AssistantMessage`: A message of the LLM.\n",
    "\n",
    "Parameters are defined within the message string using the following syntax: `{{?param_name}}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f116aab-49fd-4161-934c-b675172626e4",
   "metadata": {},
   "source": [
    "Next up we create a orchestration configuration from the created objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99af79b8-1f77-4244-8528-40f1a8864e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig\n",
    "\n",
    "config = OrchestrationConfig(\n",
    "    template=template,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c83f9-a3bd-4e46-8c8b-3388b672cbad",
   "metadata": {},
   "source": [
    "Lastly, we can call the orchestration service. Note that the actual template values are now passed to the `run` method. The `TemplateValue` name parameter corresponds to the parameter name `text` provided in the user message string. The parameter `to_lang` is omitted and the default defined in the PromptTemplate is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf112f6-ba4a-4fe6-8c52-28015d81d1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive Learning with SAP. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.orchestration.service import OrchestrationService\n",
    "\n",
    "orchestration_service = OrchestrationService(\n",
    "    api_url=os.environ[\"AICORE_ORCHESTRATION_DEPLOYMENT_URL\"],\n",
    "    config=config,\n",
    ")\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"text\",\n",
    "            value=\"Interaktives Lernen mit SAP.\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb096e-68d8-4e73-bf46-74531128b612",
   "metadata": {},
   "source": [
    "# Model Harmonization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fc5aa-07f5-41b6-aba8-15527d833b7b",
   "metadata": {},
   "source": [
    "Orchestration harmonizes model usage, removing the need for prompting each model in vendor specific fashion. You can easily switch between a variety of models. Check out [this SAP Note](https://me.sap.com/notes/3437766) for further information regarding model availability. For this Jump-Start session model access is restricted to `gemini-1.5-flash` and `meta--llama3-70b-instruct` though. The code below will demonstrate how to easily switch between models, based on the templating code above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025870d1-fd8e-4330-ad5b-79ba5214f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of the text to English is:\n",
      "\n",
      "\"Interactive Learning with SAP.\"\n",
      "\n",
      "Let me know if you need any further assistance!\n"
     ]
    }
   ],
   "source": [
    "orchestration_service.config.llm = LLM(\n",
    "    name=\"meta--llama3-70b-instruct\",\n",
    ")\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"text\",\n",
    "            value=\"Interaktives Lernen mit SAP.\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f58ab-9954-4010-a884-ca045ebee737",
   "metadata": {},
   "source": [
    "You can switch between those two models and compare their responses throughout all exercises. Simply change the `name` parameter of the LLM module configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bc8ae-4ed2-468a-9094-7c842bd22ee8",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae4510-d8b7-41cc-b8c6-9b7268137411",
   "metadata": {},
   "source": [
    "Within this exercise you learned how to create a basic Orchestration pipeline that uses the Templating module. Also, you changed the model used for inference with ease. Let's explore more modules in the following exercises. Continue to [Exercise 2 - Orchestration Content Filtering](./ex2.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
